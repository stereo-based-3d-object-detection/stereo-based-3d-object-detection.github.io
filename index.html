<!DOCTYPE html>
<html>
<head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-61302010-1"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-61302010-1');
	</script>
	<title>Stereo-Based 3D Human Pose Estimation for Underwater Divers Without 3D Supervision</title>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
	<!-- <link rel='stylesheet' href='https://fonts.googleapis.com/css?family=Roboto'> -->
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	<style>
		html,body,h1,h2,h3,h4,h5,h6 {font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;}
		<!-- .cite { background:#f0f0f0; padding:10px; font-size:18px} -->
		.cite { padding:0px; background:#ffffff; font-size:18px}
		.card {border: 1px solid #ccc}
		img { margin-bottom:-6px;}
		p { font-size:18px;}
		a {text-decoration: none; color: #2196F3;}
		.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
			box-shadow:
				0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
				5px 5px 0 0px #fff, /* The second layer */
				5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
				10px 10px 0 0px #fff, /* The third layer */
				10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
				15px 15px 0 0px #fff, /* The fourth layer */
				15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
				20px 20px 0 0px #fff, /* The fifth layer */
				20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
				25px 25px 0 0px #fff, /* The fifth layer */
				25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
			margin-left: 10px;
			margin-right: 60px;
		}
	</style>
	<meta name="google-site-verification" content="8Q_ytX8FqHWcIDBc2IoJAwkJ35JRHclQw494GYdlHBE" />
</head>  
<body class="w3-white">
	<!-- Page Container -->
	<div class="w3-content w3-margin-top w3-margin-bottom" style="max-width:960px;">

		<!-- The Grid -->
		<div class="w3-row-padding">

			<!-- paper container -->	  
			<div class="w3-display-container w3-row w3-white w3-margin-bottom">
				<div class="w3-center">
					<h1>Stereo-Based 3D Object Detection</h1>
					<h5><a href="">Ying-Kun Wu</a> &emsp;&emsp; <a href="">Yi Shen</a></h5>
					<h5>Expected to be published to 2025 WACV</h5>
				</div>
				<div class="w3-center">
					<h3>[<a href="">Paper</a>] &emsp; [<a href="">Code</a>]</h3>
				</div>
		
				<hr>

				<div class="w3-center">
					<h2>Proposal</h2>
				</div>
				<p>有鑒於近年來硬體設備以及AI模型快速的進步，我們相信在不久的將來Stereo Camera會被更廣泛地使用。根據我們的了解，Stereo Image 在應用於 AI 模型時的主要挑戰在於：兩張影像之間具有大量重複資訊，以及相較於 Monocular Image 多出一倍的運算成本。然而，這些限制可望隨著硬體效能的提升而逐步克服。另外。相較於 Monocular Image，Stereo Image 能提供更豐富的資訊，特別是關鍵的深度資訊，這對於 3D Object Detection 來說是非常重要的。NVIDIA 最新的研究 <a href="https://nvlabs.github.io/FoundationStereo/">FoundationStereo</a> [1] 就證實了 Stereo Image 在深度估計方面的優勢，其準確度明顯優於 Monocular Image。此外，Stereo Image 不受限於相機的內參（intrinsics），這也是其一大優勢。綜合以上考量，我們希望開發一個以 Stereo Image 為輸入的 3D Object Detector。</p>
				<p>在我之前在Interactive Robotics and Vision Lab的研究中，我開發出了一個同樣是以Stereo Image為輸入，能夠不依靠3D Supervision就能夠還原3D人體姿態的方法。該方法充分展現了AI模型在這一任務的潛力。不過，先前的方法只考慮了單一目標（Top-Down based method)。基於這個成果，我們想延伸RT-DETR [3] 的架構，保持DETR [2] 以及我之前研究的概念，對圖片中的目標物體同時預測出物體3D box投射到圖片空間後的Keypoints以及在不同視角之間的Disparity，進而用這些模型預測出的資訊重建出一個在3D空間中的3D box，以此達到3D Object Detection的目的。如下面兩圖所示，我們主要利用了從RT-DETR中提出的Encoder，並搭配從Deformable DETR [4] 提出的deformable transformer layer來有效的預測出所需要的資訊。</p>
				<hr>

				<table class="w3-table w3-bordered">
					<tr>
						<td style="width:50%;vertical-align:middle">
							<img src="figures/hybrid-encoder.png" alt="Figure 1" style="width:100%">
							<p style="text-align:center">Figure 1: The structure of the efficient encoder developed by RT-DETR [1].</p>
						</td>
					</tr>
				</table>

				<table class="w3-table w3-bordered">
					<tr>
						<td style="width:50%;vertical-align:middle">
							<img src="figures/deformable-transformer-decoder.png" alt="Figure 2" style="width:100%">
							<p style="text-align:center">Figure 2: The structure of the deformable transformer layer [2].</p>
						</td>
					</tr>
				</table>

				<div class="w3-center">
					<h2>Demo</h2>
				</div>

				<p>3D bounding boxes ground truths are marked as red and the estimation are marked as green.<br>
					從目前的結果，可以看出我們的方法能夠順利的從stereo images中偵測出物體在3D空間中的位置（相機坐標軸）。然而，由於目前我們是採取Keypoint-Based的方法，有些形狀較不方正的物體（例如錘子）容易出現錯誤的預測。而在圖片中特徵比較不明顯的物體（例如被擋住或是形狀較小的物體）則有時後會被忽略。</p>
				<table>
					<tr style="padding:0px">
						<td style="width:100%;vertical-align:middle">
							<video controls style="width:135%;max-width:135%" autoplay="true" loop="true" playsinline="true" muted="true">
								<source src="videos/example_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
							</video>
						</td>
					</tr>
				</table>

				<table>
					<tr style="padding:0px">
						<td style="width:100%;vertical-align:middle">
							<video controls style="width:135%;max-width:135%" autoplay="true" loop="true" playsinline="true" muted="true">
								<source src="videos/example_1.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
							</video>
						</td>
					</tr>
				</table>

				<hr>

				<div class="w3-center">
					<h2>References</h2>
				</div>

				<p style="font-size:14px;">[1] B. Wen, M. Trepte, J. Aribido, J. Kautz, O. Gallo, and S. Birchfield, “FoundationStereo: Zero-Shot Stereo Matching,” arXiv preprint arXiv:2501.09898, 2025.</p>
				<p style="font-size:14px;">[2] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-End Object Detection with Transformers,” in Proc. Eur. Conf. Comput. Vis. (ECCV), 2020.</p>
				<p style="font-size:14px;">[3] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen, “DETRs Beat YOLOs on Real-time Object Detection,” CVPR, 2024.</p>
				<p style="font-size:14px;">[4] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable DETR: Deformable Transformers for End-to-End Object Detection,” in Proc. Int. Conf. Learn. Representations (ICLR), 2021.</p>
		
			</div><!-- end paper container -->
		</div><!-- End Grid -->
	</div><!-- End Page Container -->

</body>
</html>
